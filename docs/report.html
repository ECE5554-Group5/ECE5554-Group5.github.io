<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Caleb McIrvin">
<meta name="author" content="Thomas Lu">
<meta name="author" content="Alessandro Shapiro">
<meta name="dcterms.date" content="2024-12-05">

<title>Applying Conditional Control to Multiple Diffusion Generation Framework</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./proposal.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./report.html">Final Report</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Navigation</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proposal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Proposal</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./report.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Final Report</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#approach" id="toc-approach" class="nav-link" data-scroll-target="#approach"><span class="header-section-number">2</span> Approach</a>
  <ul class="collapse">
  <li><a href="#theory" id="toc-theory" class="nav-link" data-scroll-target="#theory"><span class="header-section-number">2.1</span> Theory</a></li>
  </ul></li>
  <li><a href="#experiments-and-results" id="toc-experiments-and-results" class="nav-link" data-scroll-target="#experiments-and-results"><span class="header-section-number">3</span> Experiments and Results</a>
  <ul class="collapse">
  <li><a href="#parameter-effects" id="toc-parameter-effects" class="nav-link" data-scroll-target="#parameter-effects"><span class="header-section-number">3.1</span> Parameter Effects</a></li>
  <li><a href="#qualitative-results" id="toc-qualitative-results" class="nav-link" data-scroll-target="#qualitative-results"><span class="header-section-number">3.2</span> Qualitative Results</a></li>
  <li><a href="#quantitative-experimental-arrangement" id="toc-quantitative-experimental-arrangement" class="nav-link" data-scroll-target="#quantitative-experimental-arrangement"><span class="header-section-number">3.3</span> Quantitative Experimental Arrangement</a></li>
  <li><a href="#quantitative-results" id="toc-quantitative-results" class="nav-link" data-scroll-target="#quantitative-results"><span class="header-section-number">3.4</span> Quantitative Results</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">4</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Applying Conditional Control to Multiple Diffusion Generation Framework</h1>
<p class="subtitle lead">Virginia Tech - Fall 2024 ECE 5554 Computer Vision - Course Project</p>
</div>


<div class="quarto-title-meta-author column-body">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Caleb McIrvin </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Virginia Polytechnic Institute and State University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Thomas Lu </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Virginia Polytechnic Institute and State University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Alessandro Shapiro </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Virginia Polytechnic Institute and State University
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-body">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Diffusion models have realized excellent performance in generative computer vision tasks. Classifier-free guidance frameworks like ControlNet and MultiDiffusion further improve control over image generation by incorporating conditioning information into the generation process. In this work, we seek to incorporate Canny Edge conditioning strategies from ControlNet into the MultiDiffusion generation pipeline, enabling cohesive image generation from multiple independent diffusion processes using specific spatial conditioning. The resulting framework, which we have termed MultiControl, generates visually cohesive images incorporating multiple distinct Canny Edge controls. Moreover, [quantitative results summary]. We also provide details of our PyTorch implementation of this new framework, incorporating strategies from existing ControlNet and MultiDiffusion codebases.</p>
  </div>
</div>


</header>


<div id="fig-summary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/summary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1
</figcaption>
</figure>
</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Diffusion models <span class="citation" data-cites="sohl-dicksteinDeepUnsupervisedLearning2015">[<a href="#ref-sohl-dicksteinDeepUnsupervisedLearning2015" role="doc-biblioref">1</a>]</span> realize excellent performance in generative computer vision (CV) tasks, ranging from human image generation <span class="citation" data-cites="kumarbhuniaPersonImageSynthesis2023">[<a href="#ref-kumarbhuniaPersonImageSynthesis2023" role="doc-biblioref">2</a>]</span> to optical illusion formation <span class="citation" data-cites="gengVisualAnagramsGenerating2024">[<a href="#ref-gengFactorizedDiffusionPerceptual2024" role="doc-biblioref">4</a>]</span>, to multimodal generation <span class="citation" data-cites="chenImagesThatSound2024">[<a href="#ref-chenImagesThatSound2024" role="doc-biblioref">5</a>]</span>. The success of commercial applications such as DALL-E2 <span class="citation" data-cites="DALLE">[<a href="#ref-DALLE" role="doc-biblioref">6</a>]</span> and Midjourney <span class="citation" data-cites="Midjourney">[<a href="#ref-Midjourney" role="doc-biblioref">7</a>]</span> demonstrate the demand for this emerging technology.</p>
<p>Diffusion Models (DMs) typically combine a forward and a reverse process (<a href="#fig-diffusion" class="quarto-xref">Figure&nbsp;2</a>). In Denoising Diffusion Probabilistic Models (DDPMs), one of the more popular variants of diffusion models <span class="citation" data-cites="croitoruDiffusionModelsVision2023">[<a href="#ref-croitoruDiffusionModelsVision2023" role="doc-biblioref">8</a>]</span>, the forward step consists of repeatedly adding Gaussian noise to an image across multiple time steps until the resulting image is indistinguishable from Gaussian noise. The reverse (generative) process attempts to estimate and remove the noise from the image at each time step using a Neural Network (NN). This NN is trained on images generated from applying the forward process to hundreds of millions of images<span class="citation" data-cites="croitoruDiffusionModelsVision2023">[<a href="#ref-croitoruDiffusionModelsVision2023" role="doc-biblioref">8</a>]</span>. During inference, new images can be generated by repeatedly denoisifying Gaussian noise, resulting in an image that appears to have been sampled from the training dataset.</p>
<div id="fig-diffusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/fig1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Forward and reverse process of denoising diffusion probabilistic models <span class="citation" data-cites="croitoruDiffusionModelsVision2023">[<a href="#ref-croitoruDiffusionModelsVision2023" role="doc-biblioref">8</a>]</span>
</figcaption>
</figure>
</div>
<p>Diffusion models are widely considered state-of-the-art for textually conditioned image generation <span class="citation" data-cites="bar-talMultiDiffusionFusingDiffusion2023">[<a href="#ref-bar-talMultiDiffusionFusingDiffusion2023" role="doc-biblioref">9</a>]</span>, in which a text prompt is fed into the model to direct the output generation process. However, spatial control remains challenging, even for state-of-the-art pre-trained diffusion models <span class="citation" data-cites="zhangAddingConditionalControl2023a">[<a href="#ref-zhangAddingConditionalControl2023a" role="doc-biblioref">10</a>]</span>, in part due to the stochastic nature of the sampling process.</p>
<p>Recent literature proposes incorporating spatial conditioning into the diffusion model generation process. One such framework is MultiDiffusion (MD), which unifies multiple segmentations into a single diffusion generation <span class="citation" data-cites="bar-talMultiDiffusionFusingDiffusion2023">[<a href="#ref-bar-talMultiDiffusionFusingDiffusion2023" role="doc-biblioref">9</a>]</span>. The key generation process starts from a noise generation step, after which the model will follow each crop as closely as possible to its denoised version <a href="#fig-noise" class="quarto-xref">Figure&nbsp;3 (b)</a>. This can be used for region-based generation by providing distinct segmentation masks and text prompts to produce a single cohesive result <a href="#fig-segmentation" class="quarto-xref">Figure&nbsp;11 (a)</a>. By building a new generation process on top of a pre-trained, frozen diffusion model, MD can generate visually consistent images incorporating independent, spatially defined image components.</p>
<div id="fig-multidiffusion" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multidiffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-multidiffusion" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-segmentation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/fig2.png" class="img-fluid figure-img" style="width:50.0%" data-ref-parent="fig-multidiffusion">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) A background mask associated with the text prompt “blurred image” &amp; two foreground masks are associated with “a mouse” and a “pile of books”, respectively, are used to generate a cohesive final result
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-multidiffusion" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-noise" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/multidiffusion.jpg" class="img-fluid figure-img" style="width:90.0%" data-ref-parent="fig-multidiffusion">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Multidiffusion optimization procedure
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multidiffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: MultiDiffusion image generation from segmentation masks<span class="citation" data-cites="bar-talMultiDiffusionFusingDiffusion2023">[<a href="#ref-bar-talMultiDiffusionFusingDiffusion2023" role="doc-biblioref">9</a>]</span>
</figcaption>
</figure>
</div>
<p>ControlNet (CN) is another pipeline that can control diffusion models with spatially localized input conditions. CN leverages an extra NN that modifies the behavior of a pre-trained, frozen diffusion model. This separate NN can be trained on datasets with spatial input conditions, including Canny edges, human pose estimations, or normal maps, to produce highly controlled output images. Moreover, this extra NN architecture can be trained on a limited dataset (e.g.&nbsp;100K images), which cannot be done with the original pre-trained model due to over-fitting and catastrophic forgetting <span class="citation" data-cites="zhangAddingConditionalControl2023a">[<a href="#ref-zhangAddingConditionalControl2023a" role="doc-biblioref">10</a>]</span>. As shown in <a href="#fig-controlnet" class="quarto-xref">Figure&nbsp;4</a>, the resulting pipeline can generate images with specific spatial features, even when provided with distinct text prompts.</p>
<div id="fig-controlnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-controlnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/fig3.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-controlnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: ControlNet image generation from Canny edge input<span class="citation" data-cites="zhangAddingConditionalControl2023a">[<a href="#ref-zhangAddingConditionalControl2023a" role="doc-biblioref">10</a>]</span>
</figcaption>
</figure>
</div>
<p>Although the CN framework enables specific spatial input conditions, individual image components cannot be specified with distinct text prompts. On the other hand, the MD pipeline can generate a single consistent image from multiple independent text prompts and segmentation masks, but the individual image components are not generated with the same specificity as CN. In this work, we seek to realize increased control over generated image subcomponents by incorporating Canny Edge conditioning strategies from CN into the MD generation pipeline.</p>
</section>
<section id="approach" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Approach</h1>
<p>To implement the new pipeline, which we have termed MultiControl, we combined the core MultiDiffusion logic and sampling approach with conditioning information from ControlNet.</p>
<section id="theory" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="theory"><span class="header-section-number">2.1</span> Theory</h2>
<p>Given image space <span class="math inline">\mathcal{I}</span> and condition space <span class="math inline">\mathcal{Y}</span> (eg. the text prompts) a pre-trained DM in the MD framework is represented by the mapping <span class="math display">
\Phi : \mathcal{I} \times \mathcal{Y} \longrightarrow \mathcal{I} \ \text{s.t.} \ y \in \mathcal{Y} \ \&amp; \ I_{t} \in \mathcal{I}
</span></p>
<p>Starting from noisy image <span class="math inline">I_{T}</span> the generative process, at each time step <span class="math inline">t</span> obstains the next denoised image as <span id="eq-generative-process"><span class="math display">
I_{t-1} = \Phi(I_{t} | y)
\tag{1}</span></span></p>
<p>eventually ending into a clean image <span class="math inline">I_{0}</span>.</p>
<p>The MD framework fuses multiple diffusion processes by defining a global process over global image space <span class="math inline">\mathcal{J}</span> and global condition space <span class="math inline">\mathcal{Z}</span>, which reflects the individual diffusion processes as much as possible. Specifically, it works by applying the diffusion model multiple times across different views of the image.</p>
<p>The global process, termed the MultiDiffusion process, is defined as <span class="math display">
\Psi = \mathcal{J} \times \mathcal{Z} \longrightarrow \mathcal{J} \ \text{s.t.} \ J_{t} \in \mathcal{J} \ \&amp; \ z \in \mathcal{Z}
</span></p>
<p>For each <span class="math inline">i</span>th diffusion process, mappings are defined from the global image and conditon condition spaces to I and Y. An optimization is then performed to ensure the output of each MultiDiffuser step, <span class="math inline">J_{t-1} = \Psi(J_{t}|z)</span> is similar to the output of the pretrained diffusion model (<a href="#eq-generative-process" class="quarto-xref">Equation&nbsp;1</a>).</p>
<p>Our key contribution is to seamlessly incorporate the CN model into this generation process. We demonstrate the effectiveness of our approach using a region-based text-to-image-generation setup, inspired by section 4.2 of the MultiDiffusion paper <span class="citation" data-cites="bar-talMultiDiffusionFusingDiffusion2023">[<a href="#ref-bar-talMultiDiffusionFusingDiffusion2023" role="doc-biblioref">9</a>]</span>. Under the MultiControl framework, the image space remains unchanged, but we modify the condition space by including not only text prompts but also Canny Edges. These are fed to the pre-trained ControlNet model to generate conditioning information, which can then be fed into the pre-trained diffusion model along with the textual conditioning information. This allows us to synthesize multiple spatially controllable diffusion processes into a single global MultiControl process.</p>
<p>We built our code using components from the <a href="https://github.com/lllyasviel/ControlNet">ControlNet repository</a>, and the <a href="https://github.com/omerbt/MultiDiffusion">MultiDiffusion repository</a>. Our code-base (zipped and attached) is a minimal fork of the ControlNet repository. To run our pipeline, we implemented a new gradio script, <code>gradio_multicontrol.py</code>, which presents a UI to upload images and prompts for the subcomponents and background (<a href="#fig-gradio" class="quarto-xref">Figure&nbsp;5</a>).</p>
<div id="fig-gradio" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/gradio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: MultiControl <code>gradio</code> application
</figcaption>
</figure>
</div>
<p>Each image is then processed using the <code>cv2.Canny()</code> module to generate the Canny Edge inputs used for spatial conditioning. We also automatically generate image masks to specify bounded regions for each image subcomponent. To combine the textual and spatial conditions, we use a pre-trained ControlNet model taken from <a href="https://huggingface.co/lllyasviel/ControlNet/blob/main/models/control_sd15_canny.pth">HuggingFace</a> to generate combined conditioning information. After processing and packaging the input conditions, the application calls the <code>DDIMSampler</code> class in our <code>ddim_multi_hacked.py</code> script, which we modified from the <code>ddim_hacked.py</code> script in ControlNet. This script sets up the reverse process schedule and calls the underlying model at each time step to generate the next denoised image.</p>
<p>We define our ControlLDM model wrapper in cldm.py, which is modified from the ControlNet repository and stores the core generation logic as well as the MD integration logic. Under the MD framework, we iterate over multiple views of the image to ensure visual consistency. For each view, we pass the prompt, Canny Edge conditions, and current latent to a pre-trained Stable Diffusion 1.5 checkpoint to generate and fuse the resulting generated outputs according to the MultiDiffusion approach.</p>
</section>
</section>
<section id="experiments-and-results" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Experiments and Results</h1>
<section id="parameter-effects" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="parameter-effects"><span class="header-section-number">3.1</span> Parameter Effects</h2>
<p>In this section, we provide multiple examples detailing the behavior of our MultiControl approach under multiple stress tests. We also explore the effect of varying several algorithm parameters on the quality and visual consistency of our generation results. Early on in our generation process, we noticed that many generated images tended to be visually jarring in areas with small details and contrasting colors. To counteract this issue, we took inspiration from the original ControlNet implementation by introducing adaptive control scales, which allowed us to vary the conditioning weights associated with different layers of the diffusion model. In this project, we heavily reduced the control weight for inputs later in the Stable Diffusion network, allowing it to generate high quality images while remaining visually consistent with the controlled conditioning information introduced in earlier layers (<a href="#fig-cat-man-house" class="quarto-xref">Figure&nbsp;6</a>). .</p>
<div id="fig-cat-man-house" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cat-man-house-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/cat-man-house.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cat-man-house-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: A comparison between adaptive and fixed control scales from the MultiControl approach.
</figcaption>
</figure>
</div>
<p>We also experimented with varying the number of timesteps used in the generation process (<a href="#fig-generation-time" class="quarto-xref">Figure&nbsp;7</a>). We found that modifying the number of timesteps resulted in a tradeoff between longer generation times and generation stability. At fewer than 5 timesteps, generation quality was relatively poor. Quality steadily increased until around 15 timesteps, after which additional timesteps realized diminishing returns. We also experimented with very long generation time frames (<span class="math inline">t=100</span>) to improve generations further but noticed significant blurring and color saturation.</p>
<div id="fig-generation-time" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generation-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/generation-time.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generation-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Effect of generation time on image quality
</figcaption>
</figure>
</div>
<p>To verify the MultiControl framework’s performance quantitatively, we decided to compare its performance on a given test case against MD.</p>
</section>
<section id="qualitative-results" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="qualitative-results"><span class="header-section-number">3.2</span> Qualitative Results</h2>
<p>When working with large controls, our MultiControl pipeline tends to produce high-quality visualizations. For example, in <a href="#fig-large-controls" class="quarto-xref">Figure&nbsp;8</a>, both foreground controls (the plane and the car) are closely mirrored in the generated image. Small details such as the airplane windows are captured precisely. Background features are consistent between the control and the resulting output.</p>
<div id="fig-large-controls" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-large-controls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/large-controls.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-large-controls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Large controls confer high-quality images
</figcaption>
</figure>
</div>
<p>Throughout testing, however, two key predictors of failure emerged – small controls and high numbers of controls. When conditioning on large numbers of controls, especially Canny Edges induced by smaller images, the level of detail decreases significantly. <a href="#fig-small-controls" class="quarto-xref">Figure&nbsp;9</a> shows four different input scenarios with varying numbers of controlled generations. In the leftmost image, a single, relatively large control is inserted into the image, resulting in a high-quality generation. Moving to the right, as more controls are added of decreasing size, conformance to the control canny edges decreases significantly. In the final example, with nine cats, three control images were placed in the top row of windows in the building. However, due to their small sizes, it is difficult to visually detect any cats in the top row of windows.</p>
<div id="fig-small-controls" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-small-controls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/small-controls.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-small-controls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Many smaller controls degrades image quality
</figcaption>
</figure>
</div>
</section>
<section id="quantitative-experimental-arrangement" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="quantitative-experimental-arrangement"><span class="header-section-number">3.3</span> Quantitative Experimental Arrangement</h2>
<p>To quantitatively verify the performance of MultiControl, we compared its image generation against MD for the example shown in <a href="#fig-large-controls" class="quarto-xref">Figure&nbsp;8</a>. The individual input images are shown in <a href="#fig-experiment" class="quarto-xref">Figure&nbsp;10</a>.</p>
<div id="fig-experiment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-experiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/quantitative-inputs.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-experiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10
</figcaption>
</figure>
</div>
<p>To create a comparable generation using MultiDiffusion, we extracted the image masks created for MultiControl, shown in <a href="#fig-segmentation" class="quarto-xref">Figure&nbsp;11 (a)</a>. Having MD use these shaped segmentation masks is the closest approximation we had to the multiple spatial conditions of MultiControl.</p>
<p>We generated images from both MultiControl and MultiDiffusion. For each output image, we extracted Canny Edges using the same thresholds that MultiControl used on the input images (100 and 200). We considered true edges as those in the Canny Edge image of the original car/plane image (<a href="#fig-canny-maps" class="quarto-xref">Figure&nbsp;11 (b)</a>). We considered predicted edges as those in the generated image. We considered correctly predicted edges as the predicted edges that matched the true edges. We considered these three edge types within the car and plane masks.</p>
<div id="fig-maps" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="fig-segmentation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/segmentation-maps.png" class="img-fluid figure-img" data-ref-parent="fig-maps">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Segmentation Maps
</figcaption>
</figure>
</div>
<div id="fig-canny-maps" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-canny-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/canny-edge-maps.png" class="img-fluid figure-img" data-ref-parent="fig-maps">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-canny-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Canny Edge Maps
</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Segmentation masks used by MultiControl and MD &amp; Canny edge maps used by MultiControl
</figcaption>
</figure>
</div>
<p>For each mask, we calculated two metrics. First, recall is the number of correctly predicted edges over the total number of true edges in the mask. Second, precision is the number of correctly predicted edges over the total number of predicted edges in the mask. If MultiControl produces higher recall and precision than MultiDiffusion, we can claim that MultiControl more closely matches the input spatial conditions during its image generation process than the baseline, MultiDiffusion.</p>
</section>
<section id="quantitative-results" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="quantitative-results"><span class="header-section-number">3.4</span> Quantitative Results</h2>
<p><a href="#fig-MDvsMC" class="quarto-xref">Figure&nbsp;12 (a)</a> and <a href="#fig-MCvsMD-masks" class="quarto-xref">Figure&nbsp;12 (b)</a> show the output images and corresponding Canny Edge maps generated by MultiControl and MultiDiffusion. Table 1 lists the quantitative metrics, as described in the previous section.</p>
<div id="fig-both" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-both-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="fig-MDvsMC" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-MDvsMC-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/MCvsMD.png" class="img-fluid figure-img" data-ref-parent="fig-both">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-MDvsMC-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) MultiControl (left) and MultiDiffusion (right) generation outputs.
</figcaption>
</figure>
</div>
<div id="fig-MCvsMD-masks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-MCvsMD-masks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/MCvsMD-masks.png" class="img-fluid figure-img" data-ref-parent="fig-both">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-MCvsMD-masks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) MultiControl (left) and MultiDiffusion (right) output Canny Edge maps.
</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-both-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12
</figcaption>
</figure>
</div>
<div id="tbl-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Caption MultiControl and MD Comparison of Recall and Precision for a Specific Test Case
</figcaption>
<div aria-describedby="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Input image</th>
<th>Correctly Predicted Edges</th>
<th>True Edges in Mask</th>
<th>Predicted Edges in Mask</th>
<th>Recall</th>
<th>Precision</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MultiControl</td>
<td>Car</td>
<td>1938</td>
<td>7218</td>
<td>5422</td>
<td>26.85%</td>
<td>35.74%</td>
</tr>
<tr class="even">
<td>MultiDiffusion</td>
<td>Car</td>
<td>1042</td>
<td>7218</td>
<td>5363</td>
<td>14.44%</td>
<td>19.43%</td>
</tr>
<tr class="odd">
<td>MultiControl</td>
<td>Plane</td>
<td>808</td>
<td>2375</td>
<td>3659</td>
<td>34.02%</td>
<td>22.08%</td>
</tr>
<tr class="even">
<td>MultiDiffusion</td>
<td>Plane</td>
<td>344</td>
<td>2375</td>
<td>3861</td>
<td>14.48%</td>
<td>8.91%</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>From <a href="#tbl-results" class="quarto-xref">Table&nbsp;1</a>, we see that MultiControl achieves higher precision and recall values for both foreground generations compared to MultiDiffusion. For the car image, MultiControl has 186% and 188% greater recall and precision, respectively. For the plane image, MultiControl has 235% and 248% greater recall and precision. This quantitatively demonstrates the improved effectiveness of our framework at incorporating spatial conditioning information into the generation process to produce more controllable generations.</p>
</section>
</section>
<section id="conclusion" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Conclusion</h1>
<p>This report has described a new integration of the MD and CN frameworks. By synthesizing independent ControlNet diffusion processes into one global MD process, we demonstrated visually consistent image generations that captured multiple spatial input conditions and prompts as image subcomponents.</p>
<p>However, our MultiControl framework still has several areas for improvement. First, our gradio application expects images of the same overall dimensions. Future iterations should allow images of any size, then allow the user to change the scaling and positioning of foreground images on the background image. Moreover, we have not heavily tested cross-platform performance. Future work also includes incorporating other types of input spatial conditions like human pose estimation or normal maps into our code. This would be a relatively simple addition that would extend the applications of our work.</p>
<!-- ::: {#fig-demo}
<iframe
    src="https://yuanshi-ominicontrol.hf.space"
    frameborder="0"
    width=75%
    height="450"
></iframe>

A demonstrative application showcasing the **MultiDiffusion** + **ControlNet** Model
::: -->



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="list">
<div id="ref-sohl-dicksteinDeepUnsupervisedLearning2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Sohl-Dickstein J, Weiss EA, Maheswaranathan N, Ganguli S (2015) <a href="https://arxiv.org/abs/1503.03585">Deep <span>Unsupervised Learning</span> using <span>Nonequilibrium Thermodynamics</span></a></div>
</div>
<div id="ref-kumarbhuniaPersonImageSynthesis2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Kumar Bhunia A, Khan S, Cholakkal H, et al (2023) <a href="https://doi.org/10.1109/CVPR52729.2023.00578">Person <span>Image Synthesis</span> via <span>Denoising Diffusion Model</span></a>. In: 2023 <span>IEEE</span>/<span>CVF Conference</span> on <span>Computer Vision</span> and <span>Pattern Recognition</span> (<span>CVPR</span>). IEEE, Vancouver, BC, Canada, pp 5968–5976</div>
</div>
<div id="ref-gengVisualAnagramsGenerating2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Geng D, Park I, Owens A (2024) <a href="https://doi.org/10.48550/arXiv.2311.17919">Visual <span>Anagrams</span>: <span>Generating Multi-View Optical Illusions</span> with <span>Diffusion Models</span></a></div>
</div>
<div id="ref-gengFactorizedDiffusionPerceptual2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Geng D, Park I, Owens A (2024) <a href="https://arxiv.org/abs/2404.11615">Factorized <span>Diffusion</span>: <span>Perceptual Illusions</span> by <span>Noise Decomposition</span></a></div>
</div>
<div id="ref-chenImagesThatSound2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Chen Z, Geng D, Owens A (2024) <a href="https://doi.org/10.48550/arXiv.2405.12221">Images that <span>Sound</span>: <span>Composing Images</span> and <span>Sounds</span> on a <span>Single Canvas</span></a></div>
</div>
<div id="ref-DALLE" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><span>DALL</span><span><span class="math inline">\cdot</span></span><span>E</span> 2</div>
</div>
<div id="ref-Midjourney" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Midjourney. Midjourney</div>
</div>
<div id="ref-croitoruDiffusionModelsVision2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Croitoru F-A, Hondru V, Ionescu RT, Shah M (2023) Diffusion <span>Models</span> in <span>Vision</span>: <span>A Survey</span>. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(9):10850–10869. <a href="https://doi.org/10.1109/TPAMI.2023.3261988">https://doi.org/10.1109/TPAMI.2023.3261988</a></div>
</div>
<div id="ref-bar-talMultiDiffusionFusingDiffusion2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Bar-Tal O, Yariv L, Lipman Y, Dekel T (2023) <a href="https://arxiv.org/abs/2302.08113"><span>MultiDiffusion</span>: <span>Fusing Diffusion Paths</span> for <span>Controlled Image Generation</span></a></div>
</div>
<div id="ref-zhangAddingConditionalControl2023a" class="csl-entry" role="listitem">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Zhang L, Rao A, Agrawala M (2023) <a href="https://doi.org/10.1109/ICCV51070.2023.00355">Adding <span>Conditional Control</span> to <span class="nocase">Text-to-Image Diffusion Models</span></a>. In: 2023 <span>IEEE</span>/<span>CVF International Conference</span> on <span>Computer Vision</span> (<span>ICCV</span>). pp 3813–3824</div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./proposal.html" class="pagination-link" aria-label="Project Proposal">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Project Proposal</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>