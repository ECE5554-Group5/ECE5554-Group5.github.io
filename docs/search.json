[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "",
    "text": "Diffusion models [1] have realized excellent performance in generative computer vision tasks, ranging from human image generation [2] to optical illusion formation [4], to multimodal generation [5]. With the surging popularity of commercial applications such as DALL-E2 [6] and Midjourney [7] diffusion models have been catapulted into the public eye. While diffusion models are capable of generating high-quality images and video, they can be difficult to control due to the stochastic nature of the sampling process and ambiguity in the prompts provided. In this work, we seek to realize increased control over generated image subcomponents by incorporating Canny Edge conditioning strategies from ControlNet [8] into the MultiDiffusion [9] generation pipeline.",
    "crumbs": [
      "Project Proposal"
    ]
  },
  {
    "objectID": "proposal.html#borrowed-code",
    "href": "proposal.html#borrowed-code",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "3.1 Borrowed Code",
    "text": "3.1 Borrowed Code\nBoth MultiDiffusion and ControlNet are designed to enhance the capabilities of pretrained Stable Diffusion models. In this work, we’ll experiment with a variety of baseline models but will likely use the Stable Diffusion 2 model from Stability AI due to its popularity. Additionally, as open-source implementations of both MultiDiffusion and ControlNet are available, we intend to leverage these in our solution. However, we will implement code combining these two approaches ourselves.",
    "crumbs": [
      "Project Proposal"
    ]
  },
  {
    "objectID": "proposal.html#data-collection",
    "href": "proposal.html#data-collection",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "3.2 Data Collection",
    "text": "3.2 Data Collection\nBecause we’re working with a pretrained model, significantly less data is required compared to training a model from scratch. However, we will still need to collect images to finetune our networks with, so we intend to use a tool such as Imageye [11] to download large quantities of images from Google Images. Alternatively, we may also explore making use of existing image datasets such as the Common Objects in Context (COCO) dataset or one of the Large-scale Artificial Intelligence Open Network (LAION) datasets.",
    "crumbs": [
      "Project Proposal"
    ]
  },
  {
    "objectID": "proposal.html#experiments",
    "href": "proposal.html#experiments",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "3.3 Experiments",
    "text": "3.3 Experiments\nOur experiments will consist of multiple output generations using our proposed pipeline. In general, output images should be visually consistent and reflective of both the input Canny edges and segmentation masks.\n\n3.3.1 Functionality Validation\nWe will provide at least two Canny edge inputs to our pipeline to generate a single image. Our model is successful if: 1. The output is controllable: For each distinct Canny edge input/segmentation mask, 90% of the input edges are accounted for by the Canny edge version of the corresponding region in the output image. Visual inspection should confirm that the output reflects the original Canny edge input. 2. The output is visually consistent: Visual inspection should confirm the output appears as one cohesive image with similar visual theming, lighting, and color grading throughout, rather than multiple images from different sources being joined together.\n\n\n3.3.2 Performance Comparison\nWe will test the results of our proposed pipeline against MultiDIffusion outputs without ControlNet features by comparing the visual consistency, time, and adherence to prompts. We aim to achieve improved performance compared to MultiDiffusion generations not enhanced with ControlNet Canny features.\n\n\n3.3.3 Stress Testing\nTo further understand the limitations of our proposed pipeline, we will attempt to find the lower limit on the size of the masks & the upper limit on the number of masks per image prior to breakdown of image quality caused by generations visually appearing significantly different from input Canny edges.\n\n\n3.3.4 Influence of Dataset sizes\nAs in Zheng et al (2023)[8], we will explore the effect of decreasing training dataset size on the generation of recognizable images. We anticipate that our pipeline will achieve a similar breakdown point to the baseline ControlNet training.",
    "crumbs": [
      "Project Proposal"
    ]
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "",
    "text": "Figure 1: Overview of MultiControl pipeline",
    "crumbs": [
      "Final Report"
    ]
  },
  {
    "objectID": "report.html#theory",
    "href": "report.html#theory",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "2.1 Theory",
    "text": "2.1 Theory\nGiven image space \\mathcal{I} and condition space \\mathcal{Y} (eg. the text prompts) a pre-trained DM in the MD framework is represented by the mapping \n\\Phi : \\mathcal{I} \\times \\mathcal{Y} \\longrightarrow \\mathcal{I} \\ \\text{s.t.} \\ y \\in \\mathcal{Y} \\ \\& \\ I_{t} \\in \\mathcal{I}\n\nStarting from noisy image I_{T} the generative process, at each time step t obstains the next denoised image as \nI_{t-1} = \\Phi(I_{t} | y)\n\\tag{1}\neventually ending into a clean image I_{0}.\nThe MD framework fuses multiple diffusion processes by defining a global process over global image space \\mathcal{J} and global condition space \\mathcal{Z}, which reflects the individual diffusion processes as much as possible. Specifically, it works by applying the diffusion model multiple times across different views of the image.\nThe global process, termed the MultiDiffusion process, is defined as \n\\Psi = \\mathcal{J} \\times \\mathcal{Z} \\longrightarrow \\mathcal{J} \\ \\text{s.t.} \\ J_{t} \\in \\mathcal{J} \\ \\& \\ z \\in \\mathcal{Z}\n\nFor each ith diffusion process, mappings are defined from the global image and conditon condition spaces to I and Y. An optimization is then performed to ensure the output of each MultiDiffuser step, J_{t-1} = \\Psi(J_{t}|z) is similar to the output of the pretrained diffusion model (Equation 1).\nOur key contribution is to seamlessly incorporate the CN model into this generation process. We demonstrate the effectiveness of our approach using a region-based text-to-image-generation setup, inspired by section 4.2 of the MultiDiffusion paper [9]. Under the MultiControl framework, the image space remains unchanged, but we modify the condition space by including not only text prompts but also Canny Edges. These are fed to the pre-trained ControlNet model to generate conditioning information, which can then be fed into the pre-trained diffusion model along with the textual conditioning information. This allows us to synthesize multiple spatially controllable diffusion processes into a single global MultiControl process.\nWe built our code using components from the ControlNet repository, and the MultiDiffusion repository. Our code-base (zipped and attached) is a minimal fork of the ControlNet repository. To run our pipeline, we implemented a new gradio script, gradio_multicontrol.py, which presents a UI to upload images and prompts for the subcomponents and background (Figure 5).\n\n\n\n\n\n\nFigure 5: MultiControl gradio application\n\n\n\nEach image is then processed using the cv2.Canny() module to generate the Canny Edge inputs used for spatial conditioning. We also automatically generate image masks to specify bounded regions for each image subcomponent. To combine the textual and spatial conditions, we use a pre-trained ControlNet model taken from HuggingFace to generate combined conditioning information. After processing and packaging the input conditions, the application calls the DDIMSampler class in our ddim_multi_hacked.py script, which we modified from the ddim_hacked.py script in ControlNet. This script sets up the reverse process schedule and calls the underlying model at each time step to generate the next denoised image.\nWe define our ControlLDM model wrapper in cldm.py, which is modified from the ControlNet repository and stores the core generation logic as well as the MD integration logic. Under the MD framework, we iterate over multiple views of the image to ensure visual consistency. For each view, we pass the prompt, Canny Edge conditions, and current latent to a pre-trained Stable Diffusion 1.5 checkpoint to generate and fuse the resulting generated outputs according to the MultiDiffusion approach.",
    "crumbs": [
      "Final Report"
    ]
  },
  {
    "objectID": "report.html#parameter-effects",
    "href": "report.html#parameter-effects",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "3.1 Parameter Effects",
    "text": "3.1 Parameter Effects\nIn this section, we provide multiple examples detailing the behavior of our MultiControl approach under multiple stress tests. We also explore the effect of varying several algorithm parameters on the quality and visual consistency of our generation results. Early on in our generation process, we noticed that many generated images tended to be visually jarring in areas with small details and contrasting colors. To counteract this issue, we took inspiration from the original ControlNet implementation by introducing adaptive control scales, which allowed us to vary the conditioning weights associated with different layers of the diffusion model. In this project, we heavily reduced the control weight for inputs later in the Stable Diffusion network, allowing it to generate high quality images while remaining visually consistent with the controlled conditioning information introduced in earlier layers (Figure 6). .\n\n\n\n\n\n\nFigure 6: A comparison between adaptive and fixed control scales from the MultiControl approach.\n\n\n\nWe also experimented with varying the number of timesteps used in the generation process (Figure 7). We found that modifying the number of timesteps resulted in a tradeoff between longer generation times and generation stability. At fewer than 5 timesteps, generation quality was relatively poor. Quality steadily increased until around 15 timesteps, after which additional timesteps realized diminishing returns. We also experimented with very long generation time frames (t=100) to improve generations further but noticed significant blurring and color saturation.\n\n\n\n\n\n\nFigure 7: Effect of generation time on image quality\n\n\n\nTo verify the MultiControl framework’s performance quantitatively, we decided to compare its performance on a given test case against MD.",
    "crumbs": [
      "Final Report"
    ]
  },
  {
    "objectID": "report.html#qualitative-results",
    "href": "report.html#qualitative-results",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "3.2 Qualitative Results",
    "text": "3.2 Qualitative Results\nWhen working with large controls, our MultiControl pipeline tends to produce high-quality visualizations. For example, in Figure 8, both foreground controls (the plane and the car) are closely mirrored in the generated image. Small details such as the airplane windows are captured precisely. Background features are consistent between the control and the resulting output.\n\n\n\n\n\n\nFigure 8: Large controls confer high-quality images\n\n\n\nThroughout testing, however, two key predictors of failure emerged – small controls and high numbers of controls. When conditioning on large numbers of controls, especially Canny Edges induced by smaller images, the level of detail decreases significantly. Figure 9 shows four different input scenarios with varying numbers of controlled generations. In the leftmost image, a single, relatively large control is inserted into the image, resulting in a high-quality generation. Moving to the right, as more controls are added of decreasing size, conformance to the control canny edges decreases significantly. In the final example, with nine cats, three control images were placed in the top row of windows in the building. However, due to their small sizes, it is difficult to visually detect any cats in the top row of windows.\n\n\n\n\n\n\nFigure 9: Many smaller controls degrades image quality",
    "crumbs": [
      "Final Report"
    ]
  },
  {
    "objectID": "report.html#quantitative-experimental-arrangement",
    "href": "report.html#quantitative-experimental-arrangement",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "3.3 Quantitative Experimental Arrangement",
    "text": "3.3 Quantitative Experimental Arrangement\nTo quantitatively verify the performance of MultiControl, we compared its image generation against MD for the example shown in Figure 8. The individual input images are shown in Figure 10.\n\n\n\n\n\n\nFigure 10\n\n\n\nTo create a comparable generation using MultiDiffusion, we extracted the image masks created for MultiControl, shown in Figure 11 (a). Having MD use these shaped segmentation masks is the closest approximation we had to the multiple spatial conditions of MultiControl.\nWe generated images from both MultiControl and MultiDiffusion. For each output image, we extracted Canny Edges using the same thresholds that MultiControl used on the input images (100 and 200). We considered true edges as those in the Canny Edge image of the original car/plane image (Figure 11 (b)). We considered predicted edges as those in the generated image. We considered correctly predicted edges as the predicted edges that matched the true edges. We considered these three edge types within the car and plane masks.\n\n\n\n\n\n\n\n\n\n(a) Segmentation Maps\n\n\n\n\n\n\n\n\n\n(b) Canny Edge Maps\n\n\n\n\n\nFigure 11: Segmentation masks used by MultiControl and MD & Canny edge maps used by MultiControl\n\n\n\nFor each mask, we calculated two metrics. First, recall is the number of correctly predicted edges over the total number of true edges in the mask. Second, precision is the number of correctly predicted edges over the total number of predicted edges in the mask. If MultiControl produces higher recall and precision than MultiDiffusion, we can claim that MultiControl more closely matches the input spatial conditions during its image generation process than the baseline, MultiDiffusion.",
    "crumbs": [
      "Final Report"
    ]
  },
  {
    "objectID": "report.html#quantitative-results",
    "href": "report.html#quantitative-results",
    "title": "Applying Conditional Control to Multiple Diffusion Generation Framework",
    "section": "3.4 Quantitative Results",
    "text": "3.4 Quantitative Results\nFigure 12 (a) and Figure 12 (b) show the output images and corresponding Canny Edge maps generated by MultiControl and MultiDiffusion. Table 1 lists the quantitative metrics, as described in the previous section.\n\n\n\n\n\n\n\n\n\n(a) MultiControl (left) and MultiDiffusion (right) generation outputs.\n\n\n\n\n\n\n\n\n\n(b) MultiControl (left) and MultiDiffusion (right) output Canny Edge maps.\n\n\n\n\n\nFigure 12\n\n\n\n\n\n\nTable 1: Caption MultiControl and MD Comparison of Recall and Precision for a Specific Test Case\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput image\nCorrectly Predicted Edges\nTrue Edges in Mask\nPredicted Edges in Mask\nRecall\nPrecision\n\n\n\n\nMultiControl\nCar\n1938\n7218\n5422\n26.85%\n35.74%\n\n\nMultiDiffusion\nCar\n1042\n7218\n5363\n14.44%\n19.43%\n\n\nMultiControl\nPlane\n808\n2375\n3659\n34.02%\n22.08%\n\n\nMultiDiffusion\nPlane\n344\n2375\n3861\n14.48%\n8.91%\n\n\n\n\n\n\nFrom Table 1, we see that MultiControl achieves higher precision and recall values for both foreground generations compared to MultiDiffusion. For the car image, MultiControl has 186% and 188% greater recall and precision, respectively. For the plane image, MultiControl has 235% and 248% greater recall and precision. This quantitatively demonstrates the improved effectiveness of our framework at incorporating spatial conditioning information into the generation process to produce more controllable generations.",
    "crumbs": [
      "Final Report"
    ]
  }
]