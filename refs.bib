@misc{bar-talMultiDiffusionFusingDiffusion2023,
  title = {{{MultiDiffusion}}: {{Fusing Diffusion Paths}} for {{Controlled Image Generation}}},
  shorttitle = {{{MultiDiffusion}}},
  author = {{Bar-Tal}, Omer and Yariv, Lior and Lipman, Yaron and Dekel, Tali},
  year = {2023},
  month = feb,
  number = {arXiv:2302.08113},
  eprint = {2302.08113},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long retraining and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/F4A9F63Y/Bar-Tal et al. - 2023 - MultiDiffusion Fusing Diffusion Paths for Control.pdf}
}

@misc{chenImagesThatSound2024,
  title = {Images That {{Sound}}: {{Composing Images}} and {{Sounds}} on a {{Single Canvas}}},
  shorttitle = {Images That {{Sound}}},
  author = {Chen, Ziyang and Geng, Daniel and Owens, Andrew},
  year = {2024},
  month = may,
  number = {arXiv:2405.12221},
  eprint = {2405.12221},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.12221},
  urldate = {2024-09-25},
  abstract = {Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/ams557-macos/.zotero/storage/B3UMBV6E/Chen et al. - 2024 - Images that Sound Composing Images and Sounds on .pdf;/Users/ams557-macos/.zotero/storage/CAPPTR7W/2405.html}
}

@article{croitoruDiffusionModelsVision2023,
  title = {Diffusion {{Models}} in {{Vision}}: {{A Survey}}},
  shorttitle = {Diffusion {{Models}} in {{Vision}}},
  author = {Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
  year = {2023},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {9},
  pages = {10850--10869},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3261988},
  urldate = {2024-10-05},
  abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
  keywords = {Computational modeling,Computer vision,Data models,deep generative modeling,Denoising diffusion models,diffusion models,image generation,Mathematical models,noise conditioned score networks,Noise reduction,score-based models,Task analysis,Training},
  file = {/Users/ams557-macos/.zotero/storage/DKJP5NYY/Croitoru et al. - 2023 - Diffusion Models in Vision A Survey.pdf;/Users/ams557-macos/.zotero/storage/7BYVDU6E/10081412.html}
}

@misc{DALLE,
  title = {{{DALL}}{$\cdot$}{{E}} 2},
  urldate = {2024-10-07},
  abstract = {DALL{$\cdot$}E 2 is an AI system that can create realistic images and art from a description in natural language.},
  howpublished = {https://openai.com/index/dall-e-2/},
  langid = {american},
  file = {/Users/ams557-macos/.zotero/storage/N72U94PU/dall-e-2.html}
}

@misc{gengFactorizedDiffusionPerceptual2024,
  title = {Factorized {{Diffusion}}: {{Perceptual Illusions}} by {{Noise Decomposition}}},
  shorttitle = {Factorized {{Diffusion}}},
  author = {Geng, Daniel and Park, Inbum and Owens, Andrew},
  year = {2024},
  month = apr,
  number = {arXiv:2404.11615},
  eprint = {2404.11615},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {Given a factorization of an image into a sum of linear components, we present a zero-shot method to control each individual component through diffusion model sampling. For example, we can decompose an image into low and high spatial frequencies and condition these components on different text prompts. This produces hybrid images, which change appearance depending on viewing distance. By decomposing an image into three frequency subbands, we can generate hybrid images with three prompts. We also use a decomposition into grayscale and color components to produce images whose appearance changes when they are viewed in grayscale, a phenomena that naturally occurs under dim lighting. And we explore a decomposition by a motion blur kernel, which produces images that change appearance under motion blurring. Our method works by denoising with a composite noise estimate, built from the components of noise estimates conditioned on different prompts. We also show that for certain decompositions, our method recovers prior approaches to compositional generation and spatial control. Finally, we show that we can extend our approach to generate hybrid images from real images. We do this by holding one component fixed and generating the remaining components, effectively solving an inverse problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/DRM84SJZ/Geng et al. - 2024 - Factorized Diffusion Perceptual Illusions by Nois.pdf}
}

@misc{gengVisualAnagramsGenerating2024,
  title = {Visual {{Anagrams}}: {{Generating Multi-View Optical Illusions}} with {{Diffusion Models}}},
  shorttitle = {Visual {{Anagrams}}},
  author = {Geng, Daniel and Park, Inbum and Owens, Andrew},
  year = {2024},
  month = apr,
  number = {arXiv:2311.17919},
  eprint = {2311.17919},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.17919},
  urldate = {2024-09-25},
  abstract = {We address the problem of synthesizing multi-view optical illusions: images that change appearance upon a transformation, such as a flip or rotation. We propose a simple, zero-shot method for obtaining these illusions from off-the-shelf text-to-image diffusion models. During the reverse diffusion process, we estimate the noise from different views of a noisy image, and then combine these noise estimates together and denoise the image. A theoretical analysis suggests that this method works precisely for views that can be written as orthogonal transformations, of which permutations are a subset. This leads to the idea of a visual anagram--an image that changes appearance under some rearrangement of pixels. This includes rotations and flips, but also more exotic pixel permutations such as a jigsaw rearrangement. Our approach also naturally extends to illusions with more than two views. We provide both qualitative and quantitative results demonstrating the effectiveness and flexibility of our method. Please see our project webpage for additional visualizations and results: https://dangeng.github.io/visual\_anagrams/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/XEJN855H/Geng et al. - 2024 - Visual Anagrams Generating Multi-View Optical Ill.pdf;/Users/ams557-macos/.zotero/storage/SYCV9QNI/2311.html}
}

@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11239},
  urldate = {2024-10-07},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ams557-macos/.zotero/storage/NZ8TAUD6/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/Users/ams557-macos/.zotero/storage/XDAIVH2Y/2006.html}
}

@misc{ImageyeImageDownloader,
  title = {Imageye - {{Image Downloader}}},
  urldate = {2024-10-07},
  abstract = {With this Image Downloader, you can find, browse, and download all the images present on any web page.},
  howpublished = {https://www.imageye.net/},
  langid = {english},
  file = {/Users/ams557-macos/.zotero/storage/N2M9SWM5/www.imageye.net.html}
}

@misc{juBrushNetPlugPlayImage2024,
  title = {{{BrushNet}}: {{A Plug-and-Play Image Inpainting Model}} with {{Decomposed Dual-Branch Diffusion}}},
  shorttitle = {{{BrushNet}}},
  author = {Ju, Xuan and Liu, Xian and Wang, Xintao and Bian, Yuxuan and Shan, Ying and Xu, Qiang},
  year = {2024},
  month = mar,
  number = {arXiv:2403.06976},
  eprint = {2403.06976},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-25},
  abstract = {Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs). Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality. Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches. This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion. Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes. Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment. Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/MJFSRIUK/Ju et al. - 2024 - BrushNet A Plug-and-Play Image Inpainting Model w.pdf}
}

@misc{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-10-05},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ams557-macos/.zotero/storage/V62JW8SM/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf}
}

@inproceedings{kumarbhuniaPersonImageSynthesis2023,
  title = {Person {{Image Synthesis}} via {{Denoising Diffusion Model}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kumar Bhunia, Ankan and Khan, Salman and Cholakkal, Hisham and Anwer, Rao Muhammad and Laaksonen, Jorma and Shah, Mubarak and Khan, Fahad Shahbaz},
  year = {2023},
  month = jun,
  pages = {5968--5976},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.00578},
  urldate = {2024-10-07},
  abstract = {The pose-guided person image generation task requires synthesizing photorealistic images of humans in arbitrary poses. The existing approaches use generative adversarial networks that do not necessarily maintain realistic textures or need dense correspondences that struggle to handle complex deformations and severe occlusions. In this work, we show how denoising diffusion models can be applied for high-fidelity person image synthesis with strong sample diversity and enhanced mode coverage of the learnt data distribution. Our proposed Person Image Diffusion Model (PIDM) disintegrates the complex transfer problem into a series of simpler forward-backward denoising steps. This helps in learning plausible sourceto-target transformation trajectories that result in faithful textures and undistorted appearance details. We introduce a `texture diffusion module' based on cross-attention to accurately model the correspondences between appearance and pose information available in source and target images. Further, we propose `disentangled classifier-free guidance' to ensure close resemblance between the conditional inputs and the synthesized output in terms of both pose and appearance information. Our extensive results on two large-scale benchmarks and a user study demonstrate the photorealism of our proposed approach under challenging scenarios. We also show how our generated images can help in downstream tasks. Code is available at https://github.com/ankanbhunia/PIDM.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350301298},
  langid = {english},
  file = {/Users/ams557-macos/.zotero/storage/YWLFZFNF/Kumar Bhunia et al. - 2023 - Person Image Synthesis via Denoising Diffusion Mod.pdf}
}

@misc{leeSyncDiffusionCoherentMontage2023,
  title = {{{SyncDiffusion}}: {{Coherent Montage}} via {{Synchronized Joint Diffusions}}},
  shorttitle = {{{SyncDiffusion}}},
  author = {Lee, Yuseung and Kim, Kunho and Kim, Hyunjin and Sung, Minhyuk},
  year = {2023},
  month = oct,
  number = {arXiv:2306.05178},
  eprint = {2306.05178},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.05178},
  urldate = {2024-10-09},
  abstract = {The remarkable capabilities of pretrained image diffusion models have been utilized not only for generating fixed-size images but also for creating panoramas. However, naive stitching of multiple images often results in visible seams. Recent techniques have attempted to address this issue by performing joint diffusions in multiple windows and averaging latent features in overlapping regions. However, these approaches, which focus on seamless montage generation, often yield incoherent outputs by blending different scenes within a single image. To overcome this limitation, we propose SyncDiffusion, a plug-and-play module that synchronizes multiple diffusions through gradient descent from a perceptual similarity loss. Specifically, we compute the gradient of the perceptual loss using the predicted denoised images at each denoising step, providing meaningful guidance for achieving coherent montages. Our experimental results demonstrate that our method produces significantly more coherent outputs compared to previous methods (66.35\% vs. 33.65\% in our user study) while still maintaining fidelity (as assessed by GIQA) and compatibility with the input prompt (as measured by CLIP score). We further demonstrate the versatility of our method across three plug-and-play applications: layout-guided image generation, conditional image generation and 360-degree panorama generation. Our project page is at https://syncdiffusion.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/27QAPF9U/Lee et al. - 2023 - SyncDiffusion Coherent Montage via Synchronized J.pdf;/Users/ams557-macos/.zotero/storage/IAPE38U2/2306.html}
}

@misc{liuCompositionalVisualGeneration2023,
  title = {Compositional {{Visual Generation}} with {{Composable Diffusion Models}}},
  author = {Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B.},
  year = {2023},
  month = jan,
  number = {arXiv:2206.01714},
  eprint = {2206.01714},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {Large text-guided diffusion models, such as DALL-E 2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALL-E 2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/ams557-macos/.zotero/storage/AM6HCQSB/Liu et al. - 2023 - Compositional Visual Generation with Composable Di.pdf}
}

@misc{Midjourney,
  title = {Midjourney},
  journal = {Midjourney},
  urldate = {2024-10-07},
  abstract = {An independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.},
  howpublished = {https://www.midjourney.com/website},
  file = {/Users/ams557-macos/.zotero/storage/Q947ZAG6/home.html}
}

@misc{podellSDXLImprovingLatent2023,
  title = {{{SDXL}}: {{Improving Latent Diffusion Models}} for {{High-Resolution Image Synthesis}}},
  shorttitle = {{{SDXL}}},
  author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  year = {2023},
  month = jul,
  number = {arXiv:2307.01952},
  eprint = {2307.01952},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.01952},
  urldate = {2024-10-07},
  abstract = {We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/IGMPU3BB/Podell et al. - 2023 - SDXL Improving Latent Diffusion Models for High-R.pdf}
}

@misc{renMoveAnythingLayered2024,
  title = {Move {{Anything}} with {{Layered Scene Diffusion}}},
  author = {Ren, Jiawei and Xu, Mengmeng and Wu, Jui-Chieh and Liu, Ziwei and Xiang, Tao and Toisoul, Antoine},
  year = {2024},
  month = apr,
  number = {arXiv:2404.07178},
  eprint = {2404.07178},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.07178},
  urldate = {2024-10-09},
  abstract = {Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts? Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process. In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process. Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts. Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing. Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images. Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/TEK2PUIP/Ren et al. - 2024 - Move Anything with Layered Scene Diffusion.pdf;/Users/ams557-macos/.zotero/storage/2THL2HF4/2404.html}
}

@misc{rombachHighResolutionImageSynthesis2022a,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2024-10-07},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ams557-macos/.zotero/storage/G8ZP3IDC/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf}
}

@misc{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {arXiv},
  urldate = {2024-10-09},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/ams557-macos/.zotero/storage/WQ9FX6HA/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}

@inproceedings{zhangAddingConditionalControl2023a,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  year = {2023},
  month = oct,
  pages = {3813--3824},
  issn = {2380-7504},
  doi = {10.1109/ICCV51070.2023.00355},
  urldate = {2024-10-09},
  abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({$<$}50k) and large ({$>$}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
  keywords = {Computer architecture,Computer vision,Image coding,Image edge detection,Image segmentation,Neural networks,Training},
  file = {/Users/ams557-macos/.zotero/storage/RRVANTUA/Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffus.pdf;/Users/ams557-macos/.zotero/storage/NXSCY4W6/10377881.html}
}
