---
subtitle: Virginia Tech - Fall 2024 ECE 5554 Computer Vision - Course Project
date: today
abstract: |
  Diffusion models have realized excellent performance in generative computer vision tasks. Classifier-free guidance frameworks like ControlNet and MultiDiffusion further improve control over image generation by incorporating conditioning information into the generation process. In this work, we seek to incorporate Canny Edge conditioning strategies from ControlNet into the MultiDiffusion generation pipeline, enabling cohesive image generation from multiple independent diffusion processes using specific spatial conditioning. The resulting framework, which we have termed MultiControl, generates visually cohesive images incorporating multiple distinct Canny Edge controls. Moreover, [quantitative results summary]. We also provide details of our PyTorch implementation of this new framework, incorporating strategies from existing ControlNet and MultiDiffusion codebases. 
---

::: {#fig-summary}
![](images/fig5.png)
:::

# Introduction

Diffusion models [@sohl-dicksteinDeepUnsupervisedLearning2015] realize excellent performance in generative computer vision (CV) tasks, ranging from human image generation [@kumarbhuniaPersonImageSynthesis2023] to optical illusion formation [@gengVisualAnagramsGenerating2024,@gengFactorizedDiffusionPerceptual2024], to multimodal generation [@chenImagesThatSound2024]. The success of commercial applications such as DALL-E2 [@DALLE] and Midjourney [@Midjourney] demonstrate the demand for this emerging technology.

Diffusion Models (DMs) typically combine a forward and a reverse process (@fig-diffusion). In Denoising Diffusion Probabilistic Models (DDPMs), one of the more popular variants of diffusion models [@croitoruDiffusionModelsVision2023], the forward step consists of repeatedly adding Gaussian noise to an image across multiple time steps until the resulting image is indistinguishable from Gaussian noise. The reverse (generative) process attempts to estimate and remove the noise from the image at each time step using a Neural Network (NN). This NN is trained on images generated from applying the forward process to hundreds of millions of images[@croitoruDiffusionModelsVision2023]. During inference, new images can be generated by repeatedly denoisifying Gaussian noise, resulting in an image that appears to have been sampled from the training dataset. 

::: {#fig-diffusion}
![](images/fig1.png)

Forward and reverse process of denoising diffusion probabilistic models [@croitoruDiffusionModelsVision2023]
:::

Diffusion models are widely considered state-of-the-art for textually conditioned image generation [@bar-talMultiDiffusionFusingDiffusion2023], in which a text prompt is fed into the model to direct the output generation process. However, spatial control remains challenging, even for state-of-the-art pre-trained diffusion models [@zhangAddingConditionalControl2023a], in part due to the stochastic nature of the sampling process. 

Recent literature proposes incorporating spatial conditioning into the diffusion model generation process. One such framework is MultiDiffusion (MD), which unifies multiple segmentations into a single diffusion generation [@bar-talMultiDiffusionFusingDiffusion2023]. The key generation process starts from a noise generation step, after which the model will follow each crop as closely as possible to its denoised version @fig-noise. This can be used for region-based generation by providing distinct segmentation masks and text prompts to produce a single cohesive result @fig-segmentation. By building a new generation process on top of a pre-trained, frozen diffusion model, MD can generate visually consistent images incorporating independent, spatially defined image components. 

::: {#fig-multidiffusion layout-ncol=}
![A background mask associated with the text prompt "blurred image" & two foreground masks are associated with "a mouse" and a "pile of books", respectively, are used to generate a cohesive final result](images/fig2.png){#fig-segmentation width=50%}

![Multidiffusion optimization procedure](images/multidiffusion.jpg){#fig-noise width=90%}

MultiDiffusion image generation from segmentation masks[@bar-talMultiDiffusionFusingDiffusion2023]
:::

ControlNet (CN) is another pipeline that can control diffusion models with spatially localized input conditions. CN leverages an extra NN that modifies the behavior of a pre-trained, frozen diffusion model. This separate NN can be trained on datasets with spatial input conditions, including Canny edges, human pose estimations, or normal maps, to produce highly controlled output images. Moreover, this extra NN architecture can be trained on a limited dataset (e.g. 100K images), which cannot be done with the original pre-trained model due to over-fitting and catastrophic forgetting [@zhangAddingConditionalControl2023a]. As shown in @fig-controlnet, the resulting pipeline can generate images with specific spatial features, even when provided with distinct text prompts. 

::: {#fig-controlnet}
![](images/fig3.png)

ControlNet image generation from Canny edge input[@zhangAddingConditionalControl2023a]
:::

Although the CN framework enables specific spatial input conditions, individual image components cannot be specified with distinct text prompts. On the other hand, the MD pipeline can generate a single consistent image from multiple independent text prompts and segmentation masks, but the individual image components are not generated with the same specificity as CN. In this work, we seek to realize increased control over generated image subcomponents by incorporating Canny Edge conditioning strategies from CN into the MD generation pipeline. 

# Approach
To implement the new pipeline, which we have termed MultiControl, we combined the core MultiDiffusion logic and sampling approach with conditioning information from ControlNet.

## Theory

Given image space $\mathcal{I}$ and condition space $\mathcal{Y}$ (eg. the text prompts) a pre-trained DM in the MD framework is represented by the mapping
$$
\Phi : \mathcal{I} \times \mathcal{Y} \longrightarrow \mathcal{I} \ \text{s.t.} \ y \in \mathcal{Y} \ \& \ I_{t} \in \mathcal{I}
$$

Starting from noisy image $I_{T}$ the generative process, at each time step $t$ obstains the next denoised image as
$$
I_{t-1} = \Phi(I_{t} | y)
$$ {#eq-generative-process}

eventually ending into a clean image $I_{0}$. 

The MD framework fuses multiple diffusion processes by defining a global process over global image space $\mathcal{J}$ and global condition space $\mathcal{Z}$, which reflects the individual diffusion processes as much as possible. Specifically, it works by applying the diffusion model multiple times across different views of the image.  

The global process, termed the MultiDiffusion process, is defined as
$$
\Psi = \mathcal{J} \times \mathcal{Z} \longrightarrow \mathcal{J} \ \text{s.t.} \ J_{t} \in \mathcal{J} \ \& \ z \in \mathcal{Z}
$$

For each $i$th diffusion process, mappings are defined from the global image and conditon condition spaces to I and Y. An optimization is then performed to ensure the output of each MultiDiffuser step, $J_{t-1} = \Psi(J_{t}|z)$ is similar to the output of the pretrained diffusion model (@eq-generative-process).

Our key contribution is to seamlessly incorporate the CN model into this generation process. We demonstrate the effectiveness of our approach using a region-based text-to-image-generation setup, inspired by section 4.2 of the MultiDiffusion paper [@bar-talMultiDiffusionFusingDiffusion2023]. Under the MultiControl framework, the image space remains unchanged, but we modify the condition space by including not only text prompts but also Canny Edges. These are fed to the pre-trained ControlNet model to generate conditioning information, which can then be fed into the pre-trained diffusion model along with the textual conditioning information. This allows us to synthesize multiple spatially controllable diffusion processes into a single global MultiControl process. 

We built our code using components from the [ControlNet repository](https://github.com/lllyasviel/ControlNet), and the [MultiDiffusion repository](https://github.com/omerbt/MultiDiffusion). Our code-base (zipped and attached) is a minimal fork of the ControlNet repository. To run our pipeline, we implemented a new gradio script, `gradio_multicontrol.py`, which presents a UI to upload images and prompts for the subcomponents and background (@fig-gradio). 

::: {#fig-gradio}
![](images/gradio.png)

MultiControl `gradio` application
:::

Each image is then processed using the `cv2.Canny()` module to generate the Canny Edge inputs used for spatial conditioning. We also automatically generate image masks to specify bounded regions for each image subcomponent. To combine the textual and spatial conditions, we use a pre-trained ControlNet model taken from [HuggingFace](https://huggingface.co/lllyasviel/ControlNet/blob/main/models/control_sd15_canny.pth) to generate combined conditioning information. After processing and packaging the input conditions, the application calls the `DDIMSampler` class in our `ddim_multi_hacked.py` script, which we modified from the `ddim_hacked.py` script in ControlNet. This script sets up the reverse process schedule and calls the underlying model at each time step to generate the next denoised image. 

We define our ControlLDM model wrapper in cldm.py, which is modified from the ControlNet repository and stores the core generation logic as well as the MD integration logic. Under the MD framework, we iterate over multiple views of the image to ensure visual consistency. For each view, we pass the prompt, Canny Edge conditions, and current latent to a pre-trained Stable Diffusion 1.5 checkpoint to generate and fuse the resulting generated outputs according to the MultiDiffusion approach. 

# Experiments and Results 

## Parameter Effects 

In this section, we provide multiple examples detailing the behavior of our MultiControl approach under multiple stress tests. We also explore the effect of varying several algorithm parameters on the quality and visual consistency of our generation results. Early on in our generation process, we noticed that many generated images tended to be visually jarring in areas with small details and contrasting colors. To counteract this issue, we took inspiration from the original ControlNet implementation by introducing adaptive control scales, which allowed us to vary the conditioning weights associated with different layers of the diffusion model. In this project, we heavily reduced the control weight for inputs later in the Stable Diffusion network, allowing it to generate high quality images while remaining visually consistent with the controlled conditioning information introduced in earlier layers (@fig-cat-man-house). . 

::: {#fig-cat-man-house}
![](images/cat-man-house.png)

A comparison between adaptive and fixed control scales from the MultiControl approach.
:::

We also experimented with varying the number of timesteps used in the generation process (@fig-generation-time). We found that modifying the number of timesteps resulted in a tradeoff between longer generation times and generation stability. At fewer than 5 timesteps, generation quality was relatively poor. Quality steadily increased until around 15 timesteps, after which additional timesteps realized diminishing returns. We also experimented with very long generation time frames ($t=100$) to improve generations further but noticed significant blurring and color saturation. 

::: {#fig-generation-time}
![](images/generation-time)

Effect of generation time on image quality
:::

To verify the MultiControl framework’s performance quantitatively, we decided to compare its performance on a given test case against MD. 

 

## Qualitative Results 

When working with large controls, our MultiControl pipeline tends to produce high-quality visualizations. For example, in @fig-large-controls, both foreground controls (the plane and the car) are closely mirrored in the generated image. Small details such as the airplane windows are captured precisely. Background features are consistent between the control and the resulting output.

::: {#fig-large-controls}
![](images/large-controls)

Large controls confer high-quality images
:::

Throughout testing, however, two key predictors of failure emerged – small controls and high numbers of controls. When conditioning on large numbers of controls, especially Canny Edges induced by smaller images, the level of detail decreases significantly. @fig-small-controls shows four different input scenarios with varying numbers of controlled generations. In the leftmost image, a single, relatively large control is inserted into the image, resulting in a high-quality generation. Moving to the right, as more controls are added of decreasing size, conformance to the control canny edges decreases significantly. In the final example, with nine cats, three control images were placed in the top row of windows in the building. However, due to their small sizes, it is difficult to visually detect any cats in the top row of windows. 

::: {#fig-small-controls}
![](images/large-controls)

Many smaller controls degrades image quality
:::

## Quantitative Experimental Arrangement  

To quantitatively verify the performance of MultiControl, we compared its image generation against MD for the example shown in @fig-large-controls. The individual input images are shown in @fig-experiment.  

::: {#fig-experiment}

![](images/quantitative-inputs)
:::

To create a comparable generation using MultiDiffusion, we extracted the image masks created for MultiControl, shown in @fig-segmentation. Having MD use these shaped segmentation masks is the closest approximation we had to the multiple spatial conditions of MultiControl. 

We generated images from both MultiControl and MultiDiffusion. For each output image, we extracted Canny Edges using the same thresholds that MultiControl used on the input images (100 and 200). We considered true edges as those in the Canny Edge image of the original car/plane image (@fig-canny-maps). We considered predicted edges as those in the generated image. We considered correctly predicted edges as the predicted edges that matched the true edges. We considered these three edge types within the car and plane masks.

::: {#fig-maps}

![Segmentation Maps](images/segmentation-maps.png){#fig-segmentation}

![Canny Edge Maps](images/canny-edge-maps.png){#fig-canny-maps}

Segmentation masks used by MultiControl and MD & Canny edge maps used by MultiControl
:::

For each mask, we calculated two metrics. First, recall is the number of correctly predicted edges over the total number of true edges in the mask. Second, precision is the number of correctly predicted edges over the total number of predicted edges in the mask. If MultiControl produces higher recall and precision than MultiDiffusion, we can claim that MultiControl more closely matches the input spatial conditions during its image generation process than the baseline, MultiDiffusion. 

## Quantitative Results 

@fig-MDvsMC and @fig-MCvsMD-masks show the output images and corresponding Canny Edge maps generated by MultiControl and MultiDiffusion. Table 1 lists the quantitative metrics, as described in the previous section. 

::: {#fig-both}

![MultiControl (left) and MultiDiffusion (right) generation outputs.](images/MCvsMD.png){#fig-MDvsMC}

![MultiControl (left) and MultiDiffusion (right) output Canny Edge maps.](images/MCvsMD-masks.png){#fig-MCvsMD-masks}

:::

| Model | Input image | Correctly Predicted Edges | True Edges in Mask | Predicted Edges in Mask | Recall | Precision| 
| --- | --- | --- | --- | --- | --- | --- | 
MultiControl | Car | 1938 | 7218 | 5422 | 26.85% | 35.74% | 
| MultiDiffusion | Car | 1042 | 7218 | 5363 | 14.44% | 19.43% | 
| MultiControl | Plane | 808 | 2375 | 3659 | 34.02% | 22.08% | 
| MultiDiffusion | Plane | 344 | 2375 | 3861 | 14.48% | 8.91% |
:Caption MultiControl and MD Comparison of Recall and Precision for a Specific Test Case   {#tbl-results} 

From @tbl-results, we see that MultiControl achieves higher precision and recall values for both foreground generations compared to MultiDiffusion. For the car image, MultiControl has 186% and 188% greater recall and precision, respectively. For the plane image, MultiControl has 235% and 248% greater recall and precision. This quantitatively demonstrates the improved effectiveness of our framework at incorporating spatial conditioning information into the generation process to produce more controllable generations.

# Conclusion 

This report has described a new integration of the MD and CN frameworks. By synthesizing independent ControlNet diffusion processes into one global MD process, we demonstrated visually consistent image generations that captured multiple spatial input conditions and prompts as image subcomponents. 

However, our MultiControl framework still has several areas for improvement. First, our gradio application expects images of the same overall dimensions. Future iterations should allow images of any size, then allow the user to change the scaling and positioning of foreground images on the background image. Moreover, we have not heavily tested cross-platform performance. Future work also includes incorporating other types of input spatial conditions like human pose estimation or normal maps into our code. This would be a relatively simple addition that would extend the applications of our work.


::: {#fig-demo}
<iframe
	src="https://yuanshi-ominicontrol.hf.space"
	frameborder="0"
	width=75%
	height="450"
></iframe>

A demonstrative application showcasing the **MultiDiffusion** + **ControlNet** Model
:::
