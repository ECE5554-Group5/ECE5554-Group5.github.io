<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Caleb McIrvin, Thomas Lu, Alessandro Shapiro">

<title>Applying Conditional Control to Multiple Diffusion Generation Framework</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="css/bootstrap.css">
</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Applying Conditional Control to Multiple Diffusion Generation Framework</h1>
<p class="subtitle lead">Virginia Tech - Fall 2024 ECE 5554 Computer Vision - Course Project</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Caleb McIrvin, Thomas Lu, Alessandro Shapiro </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="problem-statement" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Problem statement</h1>
<p>Diffusion models <span class="citation" data-cites="sohl-dicksteinDeepUnsupervisedLearning2015">[<a href="#ref-sohl-dicksteinDeepUnsupervisedLearning2015" role="doc-biblioref">1</a>]</span> have realized excellent performance in generative computer vision tasks, ranging from human image generation <span class="citation" data-cites="kumarbhuniaPersonImageSynthesis2023">[<a href="#ref-kumarbhuniaPersonImageSynthesis2023" role="doc-biblioref">2</a>]</span> to optical illusion formation <span class="citation" data-cites="gengVisualAnagramsGenerating2024">[<a href="#ref-gengFactorizedDiffusionPerceptual2024" role="doc-biblioref">4</a>]</span>, to multimodal generation <span class="citation" data-cites="chenImagesThatSound2024">[<a href="#ref-chenImagesThatSound2024" role="doc-biblioref">5</a>]</span>. With the surging popularity of commercial applications such as DALL-E2 <span class="citation" data-cites="DALLE">[<a href="#ref-DALLE" role="doc-biblioref">6</a>]</span> and Midjourney <span class="citation" data-cites="Midjourney">[<a href="#ref-Midjourney" role="doc-biblioref">7</a>]</span> diffusion models have been catapulted into the public eye. While diffusion models are capable of generating high-quality images and video, they can be difficult to control due to the stochastic nature of the sampling process and ambiguity in the prompts provided. In this work, we seek to realize increased control over generated image subcomponents by incorporating Canny Edge conditioning strategies from ControlNet <span class="citation" data-cites="zhangAddingConditionalControl2023a">[<a href="#ref-zhangAddingConditionalControl2023a" role="doc-biblioref">8</a>]</span> into the MultiDiffusion <span class="citation" data-cites="bar-talMultiDiffusionFusingDiffusion2023">[<a href="#ref-bar-talMultiDiffusionFusingDiffusion2023" role="doc-biblioref">9</a>]</span> generation pipeline.</p>
</section>
<section id="approach" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Approach</h1>
<p>Diffusion models are typically modeled as a combination of a forward and a reverse process (<a href="#fig-diffusion" class="quarto-xref">Figure&nbsp;1</a>). While multiple formulations exist, we will primarily focus on Denoising Diffusion Probabilistic Models (DDPMs), one of the more popular variants of diffusion models <span class="citation" data-cites="croitoruDiffusionModelsVision2023">[<a href="#ref-croitoruDiffusionModelsVision2023" role="doc-biblioref">10</a>]</span>. Under the DDPM framework, the forward step consists of repeatedly adding Gaussian noise to an image across multiple time steps until the resulting image is indistinguishable from Gaussian noise. The reverse (generative) process attempts to estimate and remove the noise from the image at each time step using a neural network.</p>
<div id="fig-diffusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/fig1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Forward and reverse process of denoising diffusion probabilistic models <span class="citation" data-cites="croitoruDiffusionModelsVision2023">[<a href="#ref-croitoruDiffusionModelsVision2023" role="doc-biblioref">10</a>]</span>
</figcaption>
</figure>
</div>
<p>The MultiDiffusion pipeline unifies multiple segmentations into a single diffusion generation <span class="citation" data-cites="bar-talMultiDiffusionFusingDiffusion2023">[<a href="#ref-bar-talMultiDiffusionFusingDiffusion2023" role="doc-biblioref">9</a>]</span>. In <a href="#fig-multidiffusion" class="quarto-xref">Figure&nbsp;2</a>, segmentation masks are provided for individual image components. In this example, the background mask is associated with the text prompt “blurred image”, while the two foreground masks are associated with “a mouse” and “a pile of books”, respectively. By training a new generation process on top of a pre-trained, frozen diffusion model, MultiDiffusion can generate visually consistent images provided the structure of these segmentation masks.</p>
<div id="fig-multidiffusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multidiffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/fig2.png" class="img-fluid figure-img" style="width:33.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multidiffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: MultiDiffusion image generation from segmentation masks<span class="citation" data-cites="bar-talMultiDiffusionFusingDiffusion2023">[<a href="#ref-bar-talMultiDiffusionFusingDiffusion2023" role="doc-biblioref">9</a>]</span>
</figcaption>
</figure>
</div>
<p>Conditional control architectures, such as ControlNet, on the other hand, can control diffusion models with various spatially localized input conditions, including Canny edges or human pose estimations, to produce highly specific output images that closely resemble one another, even when conditioned with additional text prompts <span class="citation" data-cites="zhangAddingConditionalControl2023a">[<a href="#ref-zhangAddingConditionalControl2023a" role="doc-biblioref">8</a>]</span> (<a href="#fig-controlnet" class="quarto-xref">Figure&nbsp;3</a>).</p>
<div id="fig-controlnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-controlnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/fig3.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-controlnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: ControlNet image generation from Canny edge input<span class="citation" data-cites="zhangAddingConditionalControl2023a">[<a href="#ref-zhangAddingConditionalControl2023a" role="doc-biblioref">8</a>]</span>
</figcaption>
</figure>
</div>
<p>We propose a pipeline that combines Canny edge conditioning from ControlNet with the multiple image segmentation masks approach of MultiDiffusion, to dynamically produce multiple highly controllable generations in a single visually coherent image <a href="#fig-pipeline" class="quarto-xref">Figure&nbsp;4</a>.</p>
<div id="fig-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/fig4.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Proposed pipeline combining ControlNet control strategy with MultiDiffusion image generation
</figcaption>
</figure>
</div>
</section>
<section id="plan-for-experiments" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Plan for Experiments</h1>
<p>The goal of our pipeline is to be able to fuse diffusion paths from two or more segmentation masks, each with their own Canny input. The resulting generations should appear visually consistent and reflect the original Canny edge inputs.</p>
<section id="borrowed-code" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="borrowed-code"><span class="header-section-number">3.1</span> Borrowed Code</h2>
<p>Both MultiDiffusion and ControlNet are designed to enhance the capabilities of pretrained Stable Diffusion models. In this work, we’ll experiment with a variety of baseline models but will likely use the <a href="https://huggingface.co/stabilityai/stable-diffusion-2">Stable Diffusion 2</a> model from Stability AI due to its popularity. Additionally, as open-source implementations of both MultiDiffusion and ControlNet are available, we intend to leverage these in our solution. However, we will implement code combining these two approaches ourselves.</p>
</section>
<section id="data-collection" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">3.2</span> Data Collection</h2>
<p>Because we’re working with a pretrained model, significantly less data is required compared to training a model from scratch. However, we will still need to collect images to finetune our networks with, so we intend to use a tool such as Imageye <span class="citation" data-cites="ImageyeImageDownloader">[<a href="#ref-ImageyeImageDownloader" role="doc-biblioref">11</a>]</span> to download large quantities of images from Google Images. Alternatively, we may also explore making use of existing image datasets such as the Common Objects in Context (COCO) dataset or one of the Large-scale Artificial Intelligence Open Network (LAION) datasets.</p>
</section>
<section id="experiments" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="experiments"><span class="header-section-number">3.3</span> Experiments</h2>
<p>Our experiments will consist of multiple output generations using our proposed pipeline. In general, output images should be visually consistent and reflective of both the input Canny edges and segmentation masks.</p>
<section id="functionality-validation" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="functionality-validation"><span class="header-section-number">3.3.1</span> Functionality Validation</h3>
<p>We will provide at least two Canny edge inputs to our pipeline to generate a single image. Our model is successful if: 1. The output is controllable: For each distinct Canny edge input/segmentation mask, 90% of the input edges are accounted for by the Canny edge version of the corresponding region in the output image. Visual inspection should confirm that the output reflects the original Canny edge input. 2. The output is visually consistent: Visual inspection should confirm the output appears as one cohesive image with similar visual theming, lighting, and color grading throughout, rather than multiple images from different sources being joined together.</p>
</section>
<section id="performance-comparison" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="performance-comparison"><span class="header-section-number">3.3.2</span> Performance Comparison</h3>
<p>We will test the results of our proposed pipeline against MultiDIffusion outputs without ControlNet features by comparing the visual consistency, time, and adherence to prompts. We aim to achieve improved performance compared to MultiDiffusion generations not enhanced with ControlNet Canny features.</p>
</section>
<section id="stress-testing" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="stress-testing"><span class="header-section-number">3.3.3</span> Stress Testing</h3>
<p>To further understand the limitations of our proposed pipeline, we will attempt to find the lower limit on the size of the masks &amp; the upper limit on the number of masks per image prior to breakdown of image quality caused by generations visually appearing significantly different from input Canny edges.</p>
</section>
<section id="influence-of-dataset-sizes" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="influence-of-dataset-sizes"><span class="header-section-number">3.3.4</span> Influence of Dataset sizes</h3>
<p>As in Zheng et al (2023)<span class="citation" data-cites="zhangAddingConditionalControl2023a">[<a href="#ref-zhangAddingConditionalControl2023a" role="doc-biblioref">8</a>]</span>, we will explore the effect of decreasing training dataset size on the generation of recognizable images. We anticipate that our pipeline will achieve a similar breakdown point to the baseline ControlNet training.</p>
<!-- ### Ambiguous Content Generation
We will test our ControlNet+MultiDiffusion pipeline on ambiguous input. Given the theoretical gains in both versatility and controllability of our model, we anticipate the generation of versitile, high-quality images from even minimal structure. -->

</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="list">
<div id="ref-sohl-dicksteinDeepUnsupervisedLearning2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Sohl-Dickstein J, Weiss EA, Maheswaranathan N, Ganguli S (2015) <a href="https://arxiv.org/abs/1503.03585">Deep <span>Unsupervised Learning</span> using <span>Nonequilibrium Thermodynamics</span></a></div>
</div>
<div id="ref-kumarbhuniaPersonImageSynthesis2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Kumar Bhunia A, Khan S, Cholakkal H, et al (2023) <a href="https://doi.org/10.1109/CVPR52729.2023.00578">Person <span>Image Synthesis</span> via <span>Denoising Diffusion Model</span></a>. In: 2023 <span>IEEE</span>/<span>CVF Conference</span> on <span>Computer Vision</span> and <span>Pattern Recognition</span> (<span>CVPR</span>). IEEE, Vancouver, BC, Canada, pp 5968–5976</div>
</div>
<div id="ref-gengVisualAnagramsGenerating2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Geng D, Park I, Owens A (2024) <a href="https://doi.org/10.48550/arXiv.2311.17919">Visual <span>Anagrams</span>: <span>Generating Multi-View Optical Illusions</span> with <span>Diffusion Models</span></a></div>
</div>
<div id="ref-gengFactorizedDiffusionPerceptual2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Geng D, Park I, Owens A (2024) <a href="https://arxiv.org/abs/2404.11615">Factorized <span>Diffusion</span>: <span>Perceptual Illusions</span> by <span>Noise Decomposition</span></a></div>
</div>
<div id="ref-chenImagesThatSound2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Chen Z, Geng D, Owens A (2024) <a href="https://doi.org/10.48550/arXiv.2405.12221">Images that <span>Sound</span>: <span>Composing Images</span> and <span>Sounds</span> on a <span>Single Canvas</span></a></div>
</div>
<div id="ref-DALLE" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><span>DALL</span><span><span class="math inline">\cdot</span></span><span>E</span> 2</div>
</div>
<div id="ref-Midjourney" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Midjourney. Midjourney</div>
</div>
<div id="ref-zhangAddingConditionalControl2023a" class="csl-entry" role="listitem">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Zhang L, Rao A, Agrawala M (2023) <a href="https://doi.org/10.1109/ICCV51070.2023.00355">Adding <span>Conditional Control</span> to <span class="nocase">Text-to-Image Diffusion Models</span></a>. In: 2023 <span>IEEE</span>/<span>CVF International Conference</span> on <span>Computer Vision</span> (<span>ICCV</span>). pp 3813–3824</div>
</div>
<div id="ref-bar-talMultiDiffusionFusingDiffusion2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Bar-Tal O, Yariv L, Lipman Y, Dekel T (2023) <a href="https://arxiv.org/abs/2302.08113"><span>MultiDiffusion</span>: <span>Fusing Diffusion Paths</span> for <span>Controlled Image Generation</span></a></div>
</div>
<div id="ref-croitoruDiffusionModelsVision2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Croitoru F-A, Hondru V, Ionescu RT, Shah M (2023) Diffusion <span>Models</span> in <span>Vision</span>: <span>A Survey</span>. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(9):10850–10869. <a href="https://doi.org/10.1109/TPAMI.2023.3261988">https://doi.org/10.1109/TPAMI.2023.3261988</a></div>
</div>
<div id="ref-ImageyeImageDownloader" class="csl-entry" role="listitem">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Imageye - <span>Image Downloader</span></div>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>