<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
  | ECE, Virginia Tech | Fall 2021: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Applying Conditional Control to Multiple Diffusion Generation Framework</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Caleb McIrvin, Thomas Lu, Alessandro Shapiro</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2024 ECE 5554 Computer Vision: Course Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
<hr>

<!-- Goal -->
<h3>Problem Statement</h3>
Diffusion models [1] have realized excellent performance in generative computer vision tasks, ranging from human image generation [2] to optical illusions [3], [4] and multimodal generation [5]. With the surging popularity of commercial applications such as DALL-E2 [6] and Midjourney [7] diffusion models have been catapulted into the public eye. While diffusion models are capable of generating high-quality images and video, they can be difficult to direct due to the stochastic nature of the sampling process and ambiguity in the prompts provided. In this work, we seek to realize increased control over generated image subcomponents by incorporating Canny Edge conditioning strategies from ControlNet [8] into the MultiDiffusion [9] generation pipeline. 
<br><br>
<!-- Approach -->
<h3>Approach</h3>
<!-- Figure 1 --> 
<figure>
  <img alt="" src="diffusion_process.png">
  <figcaption>Figure 1. Forward and reverse process of denoising diffusion probabilistic models [10].</figcaption>
</figure>
Diffusion models are typically modeled as a combination of a forward and a reverse process. While multiple formulations of diffusion models exist, we focus primarily on denoising diffusion probabilistic models (DDPMs), one of the more popular variants of diffusion models [10]. Under the DDPM framework, the forward step consists of repeatedly adding Gaussian noise to an image across multiple time steps until the resulting image is indistinguishable from Gaussian noise. The reverse (generative) process attempts to estimate and remove the noise from the image at each time step using a neural network.

<br><br>
<figure>
  <img alt="" src="multidiffusion.png">
  <figcaption>Figure 2. MultiDiffusion image generation from masks [9]. </figcaption>
</figure>

The MultiDiffusion pipeline unifies multiple segmentations into a single diffusion generation [9]. In Figure 2 above, segmentation masks are provided for individual image components. In this example, the background mask is associated with the text prompt “blurred image”, while the two foreground masks are associated with “a mouse” and “a pile of books”, respectively. By training a new generation process on top of a pre-trained, frozen diffusion model, MultiDiffusion can generate visually consistent images provided the structure of the segmentation masks, such as in Figure 2. 
<br><br>
<figure>
  <img alt="" src="controlnet.png">
  <figcaption>Figure 3. ControlNet image generation from Canny edge input [8]. </figcaption>
</figure>

Conditional control architectures, such as ControlNet, on the other hand, can control diffusion models with various spatially localized input conditions, including Canny edges or human pose estimations, to produce highly specific output images that closely resemble one another, even when conditioned with additional text prompts [8] (Figure 3).   
<br><br>

<figure>
  <img alt="" src="proposed.png">
  <figcaption>Figure 4. Proposed pipeline combining ControlNet control strategy with MultiDiffusion image generation. </figcaption>
</figure>
We propose a pipeline that combines Canny edge conditioning from ControlNet with the multiple image segmentation masks approach of MultiDiffusion, to dynamically produce multiple highly controllable generations in a single visually coherent image (Figure 4). 
<br><br>

<!-- Plan for Experiments -->
<h3>Plan for Experiments</h3>
<h4>Code</h4>
Both MultiDiffusion and ControlNet are designed to enhance the capabilities of pretrained Stable Diffusion models. In this work, we’ll experiment with a variety of baseline models but will likely use the <a href="https://huggingface.co/stabilityai/stable-diffusion-2">Stable Diffusion 2</a> model from Stability AI due to its popularity. Additionally, as open-source implementations of both MultiDiffusion and ControlNet are available, we intend to leverage these in our solution. However, we will implement code combining these two approaches ourselves. 
<h4>Data Collection</h4>
Because we’re working with a pretrained model, significantly less data is required compared to training a model from scratch. However, we will still need to collect images to finetune our networks with, so we intend to use a tool such as Imageye [11] to download large quantities of images from Google Images. Alternatively, we may also explore making use of existing image datasets such as COCO or LAION. 
<h4>Experiments</h4>
Our experiments will consist of multiple output generations using our proposed pipeline. In general, output images should be visually consistent and reflective of both the input Canny edges and segmentation masks. 

<h5>Functionality Validation</h5>
We will provide at least two Canny edge inputs to our pipeline to generate a single image. Our model is successful if: 
<ol>
  <li>The output is controllable: For each distinct Canny edge input/segmentation mask, 90% of the input edges are accounted for by the Canny edge version of the corresponding region in the output image. Visual inspection should confirm that the output reflects the original Canny edge input. </li>
  <li>The output is visually consistent: Visual inspection should confirm the output appears as one cohesive image with similar visual theming, lighting, and color grading throughout, rather than multiple images from different sources being joined together.  </li>
</ol>

<h5>Performance Comparison</h5>
We will test the results of our proposed pipeline against MultiDIffusion outputs without ControlNet features by comparing the visual consistency, time, and adherence to prompts. We aim to achieve improved performance compared to MultiDiffusion generations not enhanced with ControlNet Canny features.  

<h5>Stress Testing</h5>
To further understand the limitations of our proposed pipeline, we will attempt to find the lower limit on the size of the masks & the upper limit on the number of masks per image prior to breakdown of image quality caused by generations visually appearing significantly different from input Canny edges. 

<h5>Influence of Dataset Sizes</h5>
As in [8], we will explore the effect of decreasing training dataset size on the generation of recognizable images. We anticipate that our pipeline will achieve a similar breakdown point to the baseline ControlNet training. 

<br><br>

<!-- References -->
<h3>References</h3>
[1]	J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep Unsupervised Learning using Nonequilibrium Thermodynamics,” Nov. 18, 2015, arXiv: arXiv:1503.03585. Accessed: Oct. 09, 2024. [Online]. Available: http://arxiv.org/abs/1503.03585
<br>
[2]	A. Kumar Bhunia et al., “Person Image Synthesis via Denoising Diffusion Model,” in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada: IEEE, Jun. 2023, pp. 5968–5976. doi: 10.1109/CVPR52729.2023.00578.
<br>
[3]	D. Geng, I. Park, and A. Owens, “Factorized Diffusion: Perceptual Illusions by Noise Decomposition,” Apr. 17, 2024, arXiv: arXiv:2404.11615. Accessed: Sep. 26, 2024. [Online]. Available: http://arxiv.org/abs/2404.11615
<br>
[4]	D. Geng, I. Park, and A. Owens, “Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models,” Apr. 02, 2024, arXiv: arXiv:2311.17919. doi: 10.48550/arXiv.2311.17919.
<br>
[5]	Z. Chen, D. Geng, and A. Owens, “Images that Sound: Composing Images and Sounds on a Single Canvas,” May 20, 2024, arXiv: arXiv:2405.12221. doi: 10.48550/arXiv.2405.12221.
<br>
[6]	“DALL·E 2.” Accessed: Oct. 07, 2024. [Online]. Available: https://openai.com/index/dall-e-2/
<br>
[7]	“Midjourney,” Midjourney. Accessed: Oct. 07, 2024. [Online]. Available: https://www.midjourney.com/website
<br>
[8]	L. Zhang, A. Rao, and M. Agrawala, “Adding Conditional Control to Text-to-Image Diffusion Models,” in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Oct. 2023, pp. 3813–3824. doi: 10.1109/ICCV51070.2023.00355.
<br>
[9]	O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel, “MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation,” Feb. 16, 2023, arXiv: arXiv:2302.08113. Accessed: Sep. 26, 2024. [Online]. Available: http://arxiv.org/abs/2302.08113
<br>
[10]	F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion Models in Vision: A Survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 9, pp. 10850–10869, Sep. 2023, doi: 10.1109/TPAMI.2023.3261988.
<br>
[11]	“Imageye - Image Downloader.” Accessed: Oct. 07, 2024. [Online]. Available: https://www.imageye.net/

<br><br>


  <hr>
  <footer> 
  <p>© Caleb McIrvin, Thomas Lu, Alessandro Shapiro</p>
  </footer>
</div>
</div>

<br><br>

</body></html>